2010-04-16:
"we tested adults on the traditional Sally-Anne task with the use of an eye-tracking technique and found that close to 70% of the participants who got the standard false-belief question in the first trial (i.e. those who had no expectations about the probe question) first fixated on the wrong container (i.e. the container where they knew the toy was) before correcting this egocentric tendency and correctly responding to the question."

2010-04-23: "participants listened to the narrative and looked at the corresponding cartoon and while they were processing the standard FB question, they first looked at the wrong container almost 70% of the time, before they corrected this initial tendency and correctly responded to the question."

contrast between first-fixation and total looking time matters.  The former measures egocentric bias, the latter might measure arriving at the right answer.

Q1: Would you predict the same effect in young children (Paula mentioned that has asked perner, but sometimes implicit dev. studies report only total looking time, so we don't know where they looked first).

Q2: Is it an egocentric bias?  Could also be that looking to the location of an object is somehow related to recollection.  Cf. Daneil Richardson (UCL)'s studies with the animal popping up (Hoover & Richardson 2008).



2010-04-16: "Implicit" false belief (i.e. those where participants are not asked to make a prediction) tasks seem to give different results:

"there is a study by Ferguson and colleagues (currently in press in Language and Cognitive Processes), where they presented adult participants with an implicit false-belief task very similar to the ones used by Garnham and colleagues (in this case adults were asked to listen to a story while watching a static scene). Predictive eye-movements in this study suggest that adults are able to predict the mistaken character's behavior (i.e. by looking at the location where the object was first placed, rather than the new location where it was moved to) without showing any egocentric bias."


2010-04-23:
"In the new study [by Heather Ferguson from the University of Kent], two participants interacted while they were sitting each in front of their own computer. Both were supposedly watching the same video where an actor either moved an object from one container to another (Move condition) or simply extracted an object from a container and put it back inside (No-move condition). Then the participant who is a confederate describes the last scene in each video to the actual participant, whose eye-movements are monitored (e.g. 'Now the umbrella is in container A'). Critically, in half the move trials, the experimenter put a screen in front of the confederate's computer so that she could not see the move. In these trials the participant has 'privileged information' about the location of the object since he has watched the move and is aware that the other participant hasn't. Like in the previous study, participants' predictive eye-movements showed a significantly stronger bias towards the object's 'real' location in the shared-move trials than in the privileged-move trials. In this sense, it is clear that participants were aware of the other's perspective from early on in processing the confederate's description. However, unlike in the the above studies, participants didn't show a clear bias towards the correct (i.e. empty) container in the FB trials."

Q3: Have I understood right -- "stronger bias" to a location means total looking time was longer?  (So this isn't explicitly about first looks, it's a further part of the puzzle.)


Paula's hypothesis: including verbal descriptions matters because "language-mediated eye-movements reflect the real-time mapping of language onto representations of concurrently or previously seen objects and their locations in a scene (Altmann & Kamide, 2009; Cognition)."

Q4 explain